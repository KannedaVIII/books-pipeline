1. RESUMEN DEL PROYECTOEl proyecto implementa un pipeline de Ingeniería de Datos (ELT) en Python para consolidar información de libros de las plataformas Goodreads y Google Books en un único modelo de datos canónico y estandarizado (dim_book.parquet).El flujo de trabajo cubre la extracción mediante Web Scraping, el enriquecimiento mediante API, y la estandarización, limpieza, control de calidad y deduplicación de los datos.2. REQUISITOS PARA EL USUARIO Y LA EJECUCIÓN2.1. Requisitos de SoftwarePython: Versión 3.8 o superior.Librerías: Todas las dependencias listadas en el archivo requirements.txt deben estar instaladas.pip install -r requirements.txt
2.2. Configuración de Acceso a APIEl pipeline requiere una clave de la API de Google Books para el enriquecimiento de datos.Obtener Clave: Adquiera una clave de la Consola de Desarrolladores de Google y habilite la API de Google Books.Configuración: Cree un archivo llamado .env en la carpeta raíz del proyecto y añada su clave:# Archivo .env
GOOGLE_BOOKS_API_KEY="TU_CLAVE_DE_API_AQUÍ"
NOTA: Si la clave no se proporciona, el Ejercicio 2 ejecutará un mocking (datos simulados) y el pipeline continuará, pero la calidad de los datos de Google Books será baja.3. GUÍA DE USO (FLUJO DE TRABAJO PASO A PASO)El pipeline se ejecuta secuencialmente en tres etapas principales, que corresponden a los scripts del proyecto.PASO 1: Extracción de Goodreads (Ej. 1 - Scraping)Objetivo: Recopilar IDs y datos iniciales de libros de Goodreads.Comando:python src/ejercicio_1_goodreads_scraper.py 
Artefacto Generado: landing/goodreads_books.jsonPASO 2: Enriquecimiento con Google Books (Ej. 2 - API)Objetivo: Usar la API de Google Books para obtener metadatos ricos (editorial, fecha, idioma, precio) basados en los datos de Goodreads.Comando:python src/ejercicio_2_googlebooks_enricher.py 
Artefacto Generado: landing/googlebooks_books.csvPASO 3: Integración y Estandarización (Ej. 3 - Pipeline Principal)Objetivo: Unificar ambas fuentes, aplicar estandarización, deduplicación y generar el modelo canónico y la documentación.Comando:python src/integrate_pipeline.py 
Artefactos Generados:standard/dim_book.parquet (Modelo Canónico)standard/book_source_detail.parquet (Trazabilidad)standard/docs/quality_metrics.json (Métricas de Calidad)standard/docs/schema.md (Documentación del Esquema)4. ARTEFACTOS GENERADOS Y DIRECTORIOSEl proyecto utiliza una arquitectura de capas bien definida:DirectorioContenidoEjemplos de Archivossrc/Código fuente del pipeline.integrate_pipeline.py, utils_isbn.pylanding/Datos de origen tal cual o mínimamente procesados.goodreads_books.json, googlebooks_books.csvstandard/Capa de datos limpios, estandarizados y deduplicados.dim_book.parquet, book_source_detail.parquetstandard/docs/Documentación técnica generada automáticamente.quality_metrics.json, schema.md5. LÓGICA CLAVE DE INTEGRACIÓN5.1. Clave Canónica (book_id)El identificador primario de la capa standard se establece por orden de preferencia:ISBN-13 limpio (si está disponible).Hash generado a partir de la concatenación de Título Normalizado y Autor Principal.5.2. Regla de Supervivencia (Deduplicación)Para elegir el registro "ganador" de la tabla canónica cuando hay duplicados por book_id, se aplica esta jerarquía:Prioridad Fuente: El registro de Google Books gana sobre Goodreads.Prioridad Contenido: El registro con el Título más largo.Prioridad Temporal: El registro con la Fecha de Publicación más reciente.6. REQUISITOS NO FUNCIONALES (NFR)Robustez: El sistema implementa manejo de excepciones para fallos de red y errores HTTP (4xx, 5xx) sin detener el pipeline.Rate Limiting: Se aplican pausas temporales (time.sleep) en las llamadas a Goodreads (1.0s y 1.5s) y a Google Books (0.5s) para cumplir con las políticas de uso justo y evitar bloqueos.Calidad de Datos: Se implementan chequeos de validez de formato (idioma, moneda) y se genera un reporte de completitud para monitorizar la calidad de las fuentes de entrada.
